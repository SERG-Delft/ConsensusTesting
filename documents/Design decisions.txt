Design decisions
I decided to run rippled in a linux docker container. Running rippled on windows is poorly supported.
The idea is to create one container running the different rippled peers. They connect to the testing application through a middleware layer.
Every peer sends all its messages to that middleware using Google Protocol Buffers and gRPC. The middleware then forwards these messages to the local testing application.
The local testing application should preferrably be agnostic to the system under test. Define a general message object with which the testing application can work.
If a different consensus system were to be tested, only the middleware and containers will have to be changed. 

13-09-2021
The rippled instances will run in leaf node. The testing application will also implement the ripple p2p protocol and will run in superpeer mode.
The testing application will only relay (or not) messages from the leaf nodes. 

Potential possibilities for determining the white-box genomes in the algorithm can be running an additional ripple node per leaf node in reporting mode.
These will relay to individual node's consensus and transaction status to the testing application?

20-09-2021
The rippled instances are running in docker. Currently working on being able to establish an TLS connection from the tester to the rippled server.
This is quite difficult. Rippled uses a custom handshake to enable self-signed certificates to be used. A signature is made from the value in the 'finished' TLS message to
authenticate both sides of the connection. Unsure if this will work in java see (https://medium.com/@alexberegszaszi/why-is-it-unlikely-to-have-a-complete-alternative-implementation-of-ripple-513c8f258238).

24-09-2021
The TLS handshake did not work in Java. The needed 'finished' messages were not available in any TLS library. Found an implementation of the TLS handshake in Rust 
https://github.com/fanatid/rust-ripple-p2p/blob/master/src/app.rs, which I made work. Will continue from this to create the Protobuf objects and
store the received messages.

28-09-2021
Working implementation in Rust. Will now start multi-threading the application to allow for connections to multiple validators. 
Next is sending data on from one validator to the next.

05-10-2021
The application is now multi-threaded. It has one dedicated thread per ripple node. Each thread that receives a message from its node, sends this message to all other nodes.
There are currently still some bugs causing certain threads to crash after a while. I have also began implenting a websocket client, which is able to submit
transactions and fetch ledger/consensus info from the server. Ripple offers several client api's: gRPC, json/rpc and websocket.
I have chosen to use the websocket api, because this allows you to subscribe to certain objects such as ledgers and actively get notified on changes to that object.

07-10-2021
In order for the validators to make progress in the private network they require a similar genesis ledger.
This is because if the validators start on a random ledger, they will never agree with each other.
I will create this ledger by starting a rippled node in stand-alone mode. I will then have the regular validators start with
the same ledger.db (from the stand-alone server) in /var/lib/rippled/db and instruct the validators to continue from the genesis ledger hash.
Furthermore I suspect there is a mismatch in validator public keys, node seeds/public keys, which is causing the validators to not 
trust each other with the application as middle man.

13-10-2021
The genesis ledger is started from a common json file currently. No need for a rippled node in standalone mode.

20-10-2021
All Peers send the messages from their validator to a scheduler. This scheduler is responsible for relaying the messages.
Difficulty with keeping the network validating, with or without transactions. Trying to figure out what causes this.

21-10-2021
The validation issues might come from relaying message too slowly. Currently attempting to split a peer's underlying TCP stream into
a read and write stream. This will allow two threads to asynchronously receive and send over the TCP stream. This is, however, made difficult
due to the TLS wrapper. Might have to switch to tokio streams or something similar.

25-10-2021
I tried running a stock rippled node as the proxy. This works as intended. Problem is definitely in the rust code.

26-10-2021
I'm unable to detect a bug in the code. I will add log messages to the rippled stock proxy, to determine what messages are relayed when.

02-11-2021
I have decided to create a new design. Trying to fix the validation errors requires implementing a large part of the peer protocol in the testing application.
This is a bad idea for the following reasons. Firstly, having the proxy be the single connected peer of the validators is, one, a poor representation of the real-world
scenario in which the ripple network operates. The validators in the actual ripple network are highly connected. And two, inflexible as it allows for only one network topology.
Secondly, implementing parts of the protocol increases the complexity of the testing application and obfuscates the source of bugs (being either in the rippled code, or testing code).
Finally, implementing the peer protocol is beyond the scope of the project and will likely take longer than creating the new design.

The new design no longer has the testing application act as a peer. Instead the validators believe they are connecting to other validators, while in fact their messages
pass through the testing application. This is done by, for each connection between two validators i and j, creating two SSL sessions between validator i and the tester
and between validator j and the tester. These sessions are created by letting the tester connect to validator i using j's private key and vice versa. Every P2P connection
will still send all its messages to the scheduler. The scheduler can then determine when to send a particular message to its intended end point. 
The rippled code is altered to skip the check for authenticity of the endpoints.

This design works and keeps validating

