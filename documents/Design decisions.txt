Design decisions
I decided to run rippled in a linux docker container. Running rippled on windows is poorly supported.
The idea is to create one container running the different rippled peers. They connect to the testing application through a middleware layer.
Every peer sends all its messages to that middleware using Google Protocol Buffers and gRPC. The middleware then forwards these messages to the local testing application.
The local testing application should preferrably be agnostic to the system under test. Define a general message object with which the testing application can work.
If a different consensus system were to be tested, only the middleware and containers will have to be changed. 

13-09-2021
The rippled instances will run in leaf node. The testing application will also implement the ripple p2p protocol and will run in superpeer mode.
The testing application will only relay (or not) messages from the leaf nodes. 

Potential possibilities for determining the white-box genomes in the algorithm can be running an additional ripple node per leaf node in reporting mode.
These will relay to individual node's consensus and transaction status to the testing application?

20-09-2021
The rippled instances are running in docker. Currently working on being able to establish an TLS connection from the tester to the rippled server.
This is quite difficult. Rippled uses a custom handshake to enable self-signed certificates to be used. A signature is made from the value in the 'finished' TLS message to
authenticate both sides of the connection. Unsure if this will work in java see (https://medium.com/@alexberegszaszi/why-is-it-unlikely-to-have-a-complete-alternative-implementation-of-ripple-513c8f258238).

24-09-2021
The TLS handshake did not work in Java. The needed 'finished' messages were not available in any TLS library. Found an implementation of the TLS handshake in Rust 
https://github.com/fanatid/rust-ripple-p2p/blob/master/src/app.rs, which I made work. Will continue from this to create the Protobuf objects and
store the received messages.

28-09-2021
Working implementation in Rust. Will now start multi-threading the application to allow for connections to multiple validators. 
Next is sending data on from one validator to the next.

05-10-2021
The application is now multi-threaded. It has one dedicated thread per ripple node. Each thread that receives a message from its node, sends this message to all other nodes.
There are currently still some bugs causing certain threads to crash after a while. I have also began implenting a websocket client, which is able to submit
transactions and fetch ledger/consensus info from the server. Ripple offers several client api's: gRPC, json/rpc and websocket.
I have chosen to use the websocket api, because this allows you to subscribe to certain objects such as ledgers and actively get notified on changes to that object.

07-10-2021
In order for the validators to make progress in the private network they require a similar genesis ledger.
This is because if the validators start on a random ledger, they will never agree with each other.
I will create this ledger by starting a rippled node in stand-alone mode. I will then have the regular validators start with
the same ledger.db (from the stand-alone server) in /var/lib/rippled/db and instruct the validators to continue from the genesis ledger hash.
Furthermore I suspect there is a mismatch in validator public keys, node seeds/public keys, which is causing the validators to not 
trust each other with the application as middle man.

13-10-2021
The genesis ledger is started from a common json file currently. No need for a rippled node in standalone mode.

20-10-2021
All Peers send the messages from their validator to a scheduler. This scheduler is responsible for relaying the messages.
Difficulty with keeping the network validating, with or without transactions. Trying to figure out what causes this.

21-10-2021
The validation issues might come from relaying message too slowly. Currently attempting to split a peer's underlying TCP stream into
a read and write stream. This will allow two threads to asynchronously receive and send over the TCP stream. This is, however, made difficult
due to the TLS wrapper. Might have to switch to tokio streams or something similar.

25-10-2021
I tried running a stock rippled node as the proxy. This works as intended. Problem is definitely in the rust code.

26-10-2021
I'm unable to detect a bug in the code. I will add log messages to the rippled stock proxy, to determine what messages are relayed when.

02-11-2021
I have decided to create a new design. Trying to fix the validation errors requires implementing a large part of the peer protocol in the testing application.
This is a bad idea for the following reasons. Firstly, having the proxy be the single connected peer of the validators is, one, a poor representation of the real-world
scenario in which the ripple network operates. The validators in the actual ripple network are highly connected. And two, inflexible as it allows for only one network topology.
Secondly, implementing parts of the protocol increases the complexity of the testing application and obfuscates the source of bugs (being either in the rippled code, or testing code).
Finally, implementing the peer protocol is beyond the scope of the project and will likely take longer than creating the new design.

The new design no longer has the testing application act as a peer. Instead the validators believe they are connecting to other validators, while in fact their messages
pass through the testing application. This is done by, for each connection between two validators i and j, creating two SSL sessions between validator i and the tester
and between validator j and the tester. These sessions are created by letting the tester connect to validator i using j's private key and vice versa. Every P2P connection
will still send all its messages to the scheduler. The scheduler can then determine when to send a particular message to its intended end point. 
The rippled code is altered to skip the check for authenticity of the endpoints.

This design works and keeps validating with 2 nodes, but is too slow. Reducing thread usage solved the speed issue.

03-11-2021
The way the operating system schedules threads is very unreliable. It gives some threads more time than others causing one node to send multiple messages 
before any other nodes gets the chance to send its messages. I will investigate tokio async/await model for concurrency first. If that doesn't work
I will investigate Actix actor model.

04-11-2021
Fixed the issue. The scheduler thread was hoarding cpu resources, causing certain messages to not be delivered or sent in time.
A call to thread::yield_now() in every scheduler loop solves the issue finally.

02-12-2021
The delays are real-value endcoded. Decided to implement both gaussian and polynomial mutation as the literature suggests these provide the best results
Test harness now runs until transactions are in validated ledger

03-12-2021
Ledger are now validated only when all nodes validated the ledger. Due to UNL, one node can slack. Want to also find that.

06-12-2021
- Transactions are quite fragile to submit. If in a short time many transactions involving the same account are transmitted, the sequence number cannot be autofilled.
- The fees required for transactions in this case also quickly escalates. Setting the fee_mult_max high from its default 10 in the transaction_submit client method,
solved the issue with not submitting transactions due to insufficient fees.
- For one account a max of 10 transactions can be in the transaction queue at the same time. Either limit test harness to 10 transactions, or create multiple accounts.
The possibilities are endless. How to know when queue filled up? Only way to know is to make a custom subscription in ripple (beyond the scope).
Conclusion: Limit to 10 transactions per account, per test harness.
- Transaction fee calculations are complicated due to supporting different currencies. 
Difference between transaction level (multiplication from standard) and transaction fee (fee in drops).

When submitting more transactions in one test harness, the results can vary based on when the test case is run. This is due to the network increasing and reducing
its transaction throughput in subsequent ledgers based on the network load. Initially max 5 transactions can be applied per ledger. The maximum increase is 20% per ledger.
Hopefully this is solved after the first few test cases.

Have yet to see a proposeSeq of higher than 2. Trying to determine the theoretical maximum proposeSeq. Some info:
1. Establish takes at least 1.95s before updating positions (result => skips one heartbeat timer)
2. After that changes position max once per second.
What is the max time one round of consensus can take? Derive a max and put a check in place to alert if max is every breached.

